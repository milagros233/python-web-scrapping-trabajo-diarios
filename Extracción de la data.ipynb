{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71335e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d912da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraping(nomPuesto,cantidadPaginas):\n",
    "    #url de la pagina\n",
    "    main_url=\"https://pe.trabajosdiarios.com\"\n",
    "    front_url=\"ofertas-trabajo/de-\"+nomPuesto.replace(\" \",\"-\")\n",
    "    lista_link_avisos= []\n",
    "    nomPuesto=[]\n",
    "    nomUbicacion=[]\n",
    "    nomEmpresa=[]\n",
    "    tipoContrato=[]\n",
    "    experienciaRequerido=[]\n",
    "    educacionRequerido=[]\n",
    "    generoRequerido=[]\n",
    "    edadRequerida=[]\n",
    "    cantidadVacantes=[]\n",
    "    salarioPuesto=[]\n",
    "    \n",
    "    #repetir el numero de paginas para realizar la extración de la información\n",
    "    for pagina in range(1,cantidadPaginas+1):\n",
    "        url= main_url+\"/\"+front_url+\"?t=72&page=\"+str(pagina)\n",
    "        \n",
    "        #se le manda una solicitud y se obtiene respuesta\n",
    "        agent = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36 Vivaldi/5.3.2679.70.'}\n",
    "        main_response=requests.get(url, headers=agent,verify=False)\n",
    "        \n",
    "        #se obtiene todo el contenido de la pagina\n",
    "        main_content=main_response.text\n",
    "        \n",
    "        #BeautifulSoup en lxml para que pueda ser trabajado\n",
    "        main_soup=BeautifulSoup(main_content,'lxml')\n",
    "        \n",
    "        #buscar todas las etiquetas a con el nombre de la clase js-o-link que existen en cada aviso\n",
    "        soup_url= main_soup.find_all('a',class_='ink-secondary')\n",
    "        \n",
    "        #agregar una lista de todos los links de los avisos que aparecen en la pagina\n",
    "        \n",
    "        for link_avisos in soup_url:\n",
    "            lista_link_avisos.append(main_url+link_avisos['href'])\n",
    "        \n",
    "    \n",
    "    #iterara por cada link que se encuentran en la lista y se extraera informacion para el dataframe\n",
    "    for link_aviso in lista_link_avisos:\n",
    "        response=requests.get(link_aviso, headers=agent)\n",
    "        content=response.text\n",
    "        soup=BeautifulSoup(content,'html.parser')\n",
    "            \n",
    "        #puesto: nombre del puesto y ubicacion\n",
    "        puesto_soup= soup.find('div',class_=\"col-sm-12 fondo_blanco p-3 pb-0\" )\n",
    "        nomPuesto.append(puesto_soup.find('h1').get_text())\n",
    "        nomUbicacion.append(puesto_soup.find('div',class_='col mb-1 pb-0 ms-2 ps-3 texto_azul').get_text())\n",
    "        \n",
    "        #descripcion: nombre de la empresa, tipo de contrato, experiencia requerida, educacion requerida, genero, cantidad de vacantrs \n",
    "        descripcion_soup=soup.find('div',class_=\"row p-3 pt-1 pb-1\" )\n",
    "        nomEmpresa.append(descripcion_soup.find_all('dd',class_='col-sm-8')[0].get_text())\n",
    "        tipoContrato.append(descripcion_soup.find_all('dd',class_='col-sm-8')[2].get_text())\n",
    "        experienciaRequerido.append(descripcion_soup.find_all('dd',class_='col-sm-8')[3].get_text())\n",
    "        educacionRequerido.append(descripcion_soup.find_all('dd',class_='col-sm-8')[4].get_text())\n",
    "        generoRequerido.append(descripcion_soup.find_all('dd',class_='col-sm-8')[5].get_text())\n",
    "        if descripcion_soup.find_all('dt',class_='col-sm-4')[6].get_text() == \"Edad:\": \n",
    "            edadRequerida.append(descripcion_soup.find_all('dd',class_='col-sm-8')[6].get_text())\n",
    "            cantidadVacantes.append(descripcion_soup.find_all('dd',class_='col-sm-8')[7].get_text())\n",
    "        else: \n",
    "            cantidadVacantes.append(descripcion_soup.find_all('dd',class_='col-sm-8')[6].get_text())\n",
    "            edadRequerida.append(\"No especifica\")\n",
    "            \n",
    "        #salario del puesto\n",
    "        salario_soup=soup.find('div',class_=\"row p-3 pt-1\" )\n",
    "        if (salario_soup is None) == False : \n",
    "            salarioPuesto.append(salario_soup.find('dd',class_=\"col-sm-8\").get_text())\n",
    "        else:\n",
    "            salarioPuesto.append(\"No especifica\")\n",
    "    \n",
    "    df = pd.DataFrame({'Puesto':nomPuesto,'Empresa': nomEmpresa,'Lugar': nomUbicacion, 'Tipo contrato':tipoContrato , 'Nivel de experiencia':experienciaRequerido,\n",
    "                      'Nivel de estudio':educacionRequerido, 'Genero Requerido':generoRequerido, 'Edad Requerida':edadRequerida,\n",
    "                      'Salario': salarioPuesto, 'Cantidad Vacante': cantidadVacantes, 'Link del Puesto': lista_link_avisos})\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7436e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_scraping(\"ingeniero\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a32e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7a4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ec083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
